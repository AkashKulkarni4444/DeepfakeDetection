{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import glob\n",
    "import numpy as np\n",
    "import cv2\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frames [299, 299, 299, 299, 299, 299, 299, 299, 299, 299, 299, 300, 299, 300, 299, 299, 299, 299, 300, 300]\n",
      "Total number of videos:  20\n",
      "Average frame per video: 299.2\n"
     ]
    }
   ],
   "source": [
    "video_files =  glob.glob('D:/IIIT/IIIT_Shri_City/BTP/code/COMP90055_Research_Project-master/ResNextLSTM/Dataset/*.mp4')\n",
    "#video_files1 =  glob.glob('/content/dfdc_train_part_0/*.mp4')\n",
    "#video_files += video_files1\n",
    "frame_count = []\n",
    "for video_file in video_files:\n",
    "  cap = cv2.VideoCapture(video_file)\n",
    "  if(int(cap.get(cv2.CAP_PROP_FRAME_COUNT))<150):\n",
    "    video_files.remove(video_file)\n",
    "    continue\n",
    "  frame_count.append(int(cap.get(cv2.CAP_PROP_FRAME_COUNT)))\n",
    "print(\"frames\" , frame_count)\n",
    "print(\"Total number of videos: \" , len(frame_count))\n",
    "print('Average frame per video:',np.mean(frame_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # to extract frame\n",
    "# def frame_extract(path):\n",
    "#   vidObj = cv2.VideoCapture(path) \n",
    "#   success = 1\n",
    "#   while success:\n",
    "#       success, image = vidObj.read()\n",
    "#       if success:\n",
    "#           yield image\n",
    "\n",
    "# import os\n",
    "# # os.makedirs('F:/PYTHON10/deepfake2/Face_Only_Data')\n",
    "# import face_recognition\n",
    "# from tqdm.autonotebook import tqdm\n",
    "# # process the frames\n",
    "# def create_face_videos(path_list,out_dir):\n",
    "#   already_present_count =  glob.glob(out_dir+'*.mp4')\n",
    "#   print(\"No of videos already present \" , len(already_present_count))\n",
    "#   for path in tqdm(path_list):\n",
    "#     out_path = os.path.join(out_dir,path.split('/')[-1])\n",
    "#     file_exists = glob.glob(out_path)\n",
    "#     if(len(file_exists) != 0):\n",
    "#       print(\"File Already exists: \" , out_path)\n",
    "#       continue\n",
    "#     frames = []\n",
    "#     flag = 0\n",
    "#     face_all = []\n",
    "#     frames1 = []\n",
    "#     out = cv2.VideoWriter(out_path,cv2.VideoWriter_fourcc('M','J','P','G'), 30, (112,112))\n",
    "#     for idx,frame in enumerate(frame_extract(path)):\n",
    "#       #if(idx % 3 == 0):\n",
    "#       if(idx <= 150):\n",
    "#         frames.append(frame)\n",
    "#         if(len(frames) == 4):\n",
    "#           faces = face_recognition.batch_face_locations(frames)\n",
    "#           for i,face in enumerate(faces):\n",
    "#             if(len(face) != 0):\n",
    "#               top,right,bottom,left = face[0]\n",
    "#             try:\n",
    "#               out.write(cv2.resize(frames[i][top:bottom,left:right,:],(112,112)))\n",
    "#             except:\n",
    "#               pass\n",
    "#           frames = []\n",
    "#     try:\n",
    "#       del top,right,bottom,left\n",
    "#     except:\n",
    "#       pass\n",
    "#     out.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import glob\n",
    "import face_recognition\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def frame_extract(path):\n",
    "    vidObj = cv2.VideoCapture(path)\n",
    "    success = True\n",
    "    while success:\n",
    "        success, image = vidObj.read()\n",
    "        if success:\n",
    "            yield image\n",
    "\n",
    "# Function to create face videos\n",
    "def create_face_videos(path_list, out_dir):\n",
    "    already_present_count = len(glob.glob(os.path.join(out_dir, '*.mp4')))\n",
    "    print(\"Number of videos already present:\", already_present_count)\n",
    "    \n",
    "    for path in tqdm(path_list):\n",
    "        out_path = os.path.join(out_dir, os.path.basename(path))\n",
    "        out_path = out_path.split('.')[0] + \".mp4\"  \n",
    "        if os.path.exists(out_path):\n",
    "            print(\"File already exists:\", out_path)\n",
    "            continue\n",
    "        \n",
    "        out = None\n",
    "        frames_processed = 0\n",
    "        \n",
    "        for frame in frame_extract(path):\n",
    "            if frames_processed > 150:  \n",
    "                break\n",
    "                \n",
    "            face_locations = face_recognition.face_locations(frame)\n",
    "            if face_locations:  # If faces are detected\n",
    "                top, right, bottom, left = face_locations[0]\n",
    "                face_image = frame[top:bottom, left:right]\n",
    "                face_image_resized = cv2.resize(face_image, (112, 112))\n",
    "                \n",
    "                if out is None:\n",
    "                    out = cv2.VideoWriter(out_path, cv2.VideoWriter_fourcc(*'MP4V'), 30, (112, 112))\n",
    "                \n",
    "                out.write(face_image_resized)\n",
    "                \n",
    "            frames_processed += 1\n",
    "            \n",
    "        if out is not None:\n",
    "            out.release()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of videos already present: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [50:38<00:00, 151.93s/it]\n"
     ]
    }
   ],
   "source": [
    "create_face_videos(video_files,'D:/IIIT/IIIT_Shri_City/BTP/code/COMP90055_Research_Project-master/ResNextLSTM/Face_Only_Data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
